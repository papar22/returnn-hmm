#!crnn/rnn.py
# kate: syntax python;

#
# Based on Parnia Bahar's HMM factorization paper. 'Oloop' (out of loop) refers to the part that the hmm_factorization
# layer will be moved out of the rnn calculation loop, thus improving speed.
#

import os
from Util import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = "train"

# data
#train = {"class": "TaskEpisodicCopyDataset", "num_seqs": 1000}
#num_inputs = 10
#num_outputs = 10
train = {"class": "TaskXmlModelingDataset", "num_seqs": 1000}
num_inputs = 12
num_outputs = 12

dev = train.copy()
dev.update({"num_seqs": train["num_seqs"] // 10, "fixed_random_seed": 42})

# HMM Factorization data
vocab_size = 12
intermediary_size = 100

# network
# (also defined by num_inputs & num_outputs)
network = {
"source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 10},
"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": 1, "from": ["source_embed"] },
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": -1, "from": ["source_embed"] },
"lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": 1, "from": ["lstm0_fw", "lstm0_bw"] },
"lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": -1, "from": ["lstm0_fw", "lstm0_bw"] },

"encoder": {"class": "copy", "from": ["lstm1_fw", "lstm1_bw"]},
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": 100},  # preprocessed_attended in Blocks
"inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

"output": {"class": "rec", "from": [], "unit": {

    # Prev step processing
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': 1, 'from': ["output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},

    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 621, "initial_output": 0},  # feedback_input

    "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 200},
    "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": 100},

    # Attention
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "prev_s_transformed"], "n_out": 100},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},  # (B, enc-T, 1)
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},  # (B, enc-T, 1)
    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},

    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": 100},  # transform

    # Lexicon model
    # Prev output (B, embed_dim) --> (B, intermediary_size)
    "prev_outputs_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:target_embed"], "n_out": intermediary_size},

    # Prev state (B, hidden) --> (B, intermediary_size)
    "prev_state_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": intermediary_size},


    "base_encoder": {"class": "linear", "activation": None, "with_bias": False, "from": ["base:enc_ctx"], "n_out": intermediary_size},


    "output_prob" : {"class": "hmm_factorization",
                     "attention_weights": "att_weights",
                     "base_encoder_transformed": "base_encoder",
                     "prev_state": "prev_state_transformed",
                     "prev_outputs": "prev_outputs_transformed",
                     "threshold" : 0.01,
                     "debug": True,
                     "target": "classes",
                     "loss": "ce"},

}, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3", "optimize_move_layers_out": True},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        "debug_print": False
        }
    }
}


debug_print_layer_output_template=True

# trainer
batching = "random"
batch_size = 5000
max_seqs = 5
chunking = "0"
truncation = -1
#gradient_clip = 10
gradient_nan_inf_filter = True
adam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_control = "newbob"
learning_rate_control_relative_error_relative_lr = True
model = "/tmp/%s/crnn/%s/model" % (get_login_username(), demo_name)  # https://github.com/tensorflow/tensorflow/issues/6537
num_epochs = 100
save_interval = 20

debug_mode = True
debug_add_check_numerics_ops = True

# log
log_verbosity = 5
