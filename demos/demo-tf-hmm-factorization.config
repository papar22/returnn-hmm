#!crnn/rnn.py
# kate: syntax python;

#
# Based on Parnia Bahar's HMM factorization paper. Note: this version can only work if the hmm factorization calculation
# is done inside the rnn loop (optimize_move_layers_out=False; slightly worse speed on large datasets), or else RETURNN
# fails horrendously. See the demo-tf-hmm-factorization-oloop.config for an out of loop config.
#


import os
from Util import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = "train"

# data
#train = {"class": "TaskEpisodicCopyDataset", "num_seqs": 1000}
#num_inputs = 10
#num_outputs = 10
train = {"class": "TaskXmlModelingDataset", "num_seqs": 1000}
num_inputs = 12
num_outputs = 12

dev = train.copy()
dev.update({"num_seqs": train["num_seqs"] // 10, "fixed_random_seed": 42})

# HMM Factorization data
vocab_size = 12
intermediary_size = 100

# network
# (also defined by num_inputs & num_outputs)
network = {
"source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 10},
"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": 1, "from": ["source_embed"] },
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": -1, "from": ["source_embed"] },
"lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": 1, "from": ["lstm0_fw", "lstm0_bw"] },
"lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 100, "direction": -1, "from": ["lstm0_fw", "lstm0_bw"] },

"encoder": {"class": "copy", "from": ["lstm1_fw", "lstm1_bw"]},
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": 100},  # preprocessed_attended in Blocks
"inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

"output": {"class": "rec", "from": [], "unit": {

    # Prev step processing
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': 1, 'from': ["output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},

    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 621, "initial_output": 0},  # feedback_input

    "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 200},
    "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": 100},

    # Attention
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "prev_s_transformed"], "n_out": 100},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},  # (B, enc-T, 1)
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},  # (B, enc-T, 1)
    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},

    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": 100},  # transform

    # Lexicon model
    # Prev output (B, embed_dim) --> (B, intermediary_size)
    "prev_outputs_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:target_embed"], "n_out": intermediary_size},
    "prev_outputs_transformed_e": {"class": "expand_dims", "from": ["prev_outputs_transformed"], "axis":"spatial"},

    # Prev state (B, hidden) --> (B, intermediary_size)
    "prev_state_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": intermediary_size},
    "prev_state_transformed_e": {"class": "expand_dims", "from": ["prev_state_transformed"], "axis":"spatial"},

    # Lexicon: (B, intermediary_size) + (B, intermediary_size) + (B, J, intermediary_size) --(Auto Broadcast)--> (B, J, intermediary_size) --(softmax)--> (B, J, vocab_size)
    "base_encoder": {"class": "linear", "activation": None, "with_bias": False, "from": ["base:enc_ctx"], "n_out": intermediary_size},  # Shape fail here?
    "base_encoder_e": {"class": "expand_dims", "from": ["prev_state_transformed"], "axis":"time"},

    "lexicon_logits": {"class": "combine", "kind": "add", "from": ["base_encoder", "prev_state_transformed", "prev_outputs_transformed"], "n_out": intermediary_size},
    "lexicon_prob" : {"class": "softmax", "from": ["lexicon_logits"], "n_out": vocab_size},

    # J = enc-T
    # Mixing (in loop): (B, 1, J) x (B, J, vocab_size) --(squeeze)--> (B, vocab)
    # Mixing (when optimized out of loop): (T, B, 1, J) x (T, B, J, vocab_size) --(squeeze)--> (T, B, vocab)

    "output_prob_raw": {"class": "dot", "red1": "T", "red2": "T", "var1": "feature", "var2": "feature",
                        "from": ["att_weights", "lexicon_prob"], "debug": True},
    "output_prob" : {"class": "squeeze", "from":["output_prob_raw"], "axis": 1, "target": "classes", "loss": "ce"},

}, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3", "optimize_move_layers_out": False},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        "debug_print": False
        }
    }
}


debug_print_layer_output_template=True

# trainer
batching = "random"
batch_size = 5000
max_seqs = 5
chunking = "0"
truncation = -1
#gradient_clip = 10
gradient_nan_inf_filter = True
adam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_control = "newbob"
learning_rate_control_relative_error_relative_lr = True
model = "/tmp/%s/crnn/%s/model" % (get_login_username(), demo_name)  # https://github.com/tensorflow/tensorflow/issues/6537
num_epochs = 100
save_interval = 20

debug_mode = True
debug_add_check_numerics_ops = True

# log
log_verbosity = 5
